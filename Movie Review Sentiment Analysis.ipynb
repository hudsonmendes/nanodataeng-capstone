{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Analysis\n",
    "\n",
    "```\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "```\n",
    "\n",
    "![Movie Review Sentiment Analysis](images/header.png \"Movie Review Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "\n",
    "The present project expresses the fact **artist review sentiment** and **film review sentiment**, based on the data provided by **[IMDb](https://www.imdb.com/)** and **[TMDb](https://www.themoviedb.org/)**\n",
    "\n",
    "The goal of our **Data Pipeline** is to publish a PDF report to **S3** with the following summarised information:\n",
    "\n",
    "1. Top 10 Films\n",
    "2. Worst 10 Films\n",
    "3. Review Sentiment Distibution\n",
    "4. Top 10 Actors/Actresses in Best Reviewed Films\n",
    "5. IMDb Average Voting vs TMDb Sentiment Reviews through Years\n",
    "\n",
    "Our data sources are:\n",
    "\n",
    "* **[IMDB Large Movie Reviews Sentiment Dataset](https://www.kaggle.com/jcblaise/imdb-sentiments)**\n",
    "* **[TMDB Movies (2000-2020) with imdb_id](https://www.kaggle.com/hudsonmendes/tmdb-movies-20002020-with-imdb-id)**\n",
    "* **[TMDB Reviews, movies released from 2000 and 2020](https://www.kaggle.com/hudsonmendes/tmdb-reviews-movies-released-from-2000-and-2020)**\n",
    "* **[IMDB Cast (Links)](https://datasets.imdbws.com/title.principals.tsv.gz)**\n",
    "* **[IMDB Cast (Names)](https://datasets.imdbws.com/name.basics.tsv.gz)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Scope\n",
    "\n",
    "It's the focus of this project the implementation:\n",
    "\n",
    "1. **Infra-structure As Code** installing the entire end-to-end structure\n",
    "1. **Review Sentiment Classifier Model Trainer DAG** responsible for training our classifier\n",
    "2. **End-to-end Data Warehouse Building DAG** responsible for creating our Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Instructions\n",
    "\n",
    "In order to prepare the environment that executes the DAGs in Airflow follow the steps:\n",
    "\n",
    "1. in your shell, execute the `aws configure` command to setup your credentials\n",
    "2. run the cell bellow that will install system dependencies and then restart your kernel.\n",
    "3. immediately before the **Teardown** you will get the **Web Access** link to **Airflow**\n",
    "4. enter **Airflow** and start the **model training DAG**\n",
    "5. after the first DAG is complete, run the **data warehouse DAG**\n",
    "6. run the entire jupyter notebook apart from the **Teardown** (termination) cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:57:40.106718Z",
     "start_time": "2020-08-10T22:57:39.935363Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt-get install ssh\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:02.464268Z",
     "start_time": "2020-08-10T22:57:53.820072Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U requests\n",
    "!pip install -q -U tqdm\n",
    "!pip install -q -U pandas\n",
    "!pip install -q -U s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.089241Z",
     "start_time": "2020-08-10T22:58:02.468047Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Utilities\n",
    "\n",
    "Here we prepare some utilities that will help us visualise the raw data presented in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.114281Z",
     "start_time": "2020-08-10T22:58:04.096099Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_file(file_url, file_path, force=False):\n",
    "    file_folder = os.path.dirname(file_path)\n",
    "    if not os.path.isdir(file_folder):\n",
    "        os.makedirs(file_folder)\n",
    "    if force or not os.path.isfile(file_path):\n",
    "        file_size = int(urlopen(file_url).info().get('Content-Length', -1))\n",
    "        if os.path.exists(file_path):\n",
    "            first_byte = os.path.getsize(file_path)\n",
    "        else:\n",
    "            first_byte = 0\n",
    "        if first_byte < file_size:\n",
    "            header = {\"Range\": \"bytes=%s-%s\" % (first_byte, file_size)}\n",
    "            pbar = tqdm(total=file_size, initial=first_byte, unit='B', unit_scale=True, desc=file_url.split('/')[-1])\n",
    "            req = requests.get(file_url, headers=header, stream=True)\n",
    "            with(open(file_path, 'ab')) as f:\n",
    "                for chunk in req.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(1024)\n",
    "            pbar.close()\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IMDB Large Movie Reviews Sentiment Dataset](https://www.kaggle.com/jcblaise/imdb-sentiments)\n",
    "\n",
    "This datasets consists of movie reviews labelled as 1 (negative) or 0 (positive). Our goal is to use this information to train a Classifier model using Tensorflow, that will then be used to classify sentiment of user reviews connected to movies for which we desire to perform data analysis.\n",
    "\n",
    "The original [Kaggle Dataset](https://www.kaggle.com/jcblaise/imdb-sentiments) cannot be downloaded directly. There fore, we download a copy of it from an [S3 DataLake](https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/jcblaise/imdb-sentiment.zip) into `data/raw/reviews-sentiment`.\n",
    "\n",
    "**Important:** This data shall not be expressed directly expressed in our data warehouse's star schema. It will be used to train a tensorflow model responsible for classifying the sentiment between negative (-1) and positive (1), and that model will then be used to classify other user reviews which will then be actually presented in the facts table.\n",
    "\n",
    "| Attribute  \t| Value                                                                                                                     \t|\n",
    "|------------\t|---------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| Author     \t| Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher \t|\n",
    "| Title      \t| Learning Word Vectors for Sentiment Analysis                                                                              \t|\n",
    "| Book Title \t| Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies      \t|\n",
    "| Month      \t| June                                                                                                                      \t|\n",
    "| Year       \t| 2011                                                                                                                      \t|\n",
    "| Address    \t| Portland, Oregon, USA                                                                                                     \t|\n",
    "| Publisher  \t| Association for Computational Linguistics                                                                                 \t|\n",
    "| Pages      \t| 142--150                                                                                                                  \t|\n",
    "| URL        \t| http://www.aclweb.org/anthology/P11-1015                                                                                  \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.140957Z",
     "start_time": "2020-08-10T22:58:04.120307Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_sentiment_url  = 'https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/jcblaise/imdb-sentiment.zip'\n",
    "imdb_sentiment_path = 'data/raw/imdb-sentiment.zip'\n",
    "(imdb_sentiment_url, imdb_sentiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.160311Z",
     "start_time": "2020-08-10T22:58:04.153851Z"
    }
   },
   "outputs": [],
   "source": [
    "download_file(imdb_sentiment_url, imdb_sentiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.788737Z",
     "start_time": "2020-08-10T22:58:04.162412Z"
    }
   },
   "outputs": [],
   "source": [
    "df_raw_kaggle_train = None\n",
    "with ZipFile(imdb_sentiment_path) as zip_file:\n",
    "    df_raw_kaggle_train = pd.read_csv(\n",
    "        zip_file.open('train.csv'),\n",
    "        header=0,\n",
    "        error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.817474Z",
     "start_time": "2020-08-10T22:58:04.791171Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1024)\n",
    "df_raw_kaggle_train[df_raw_kaggle_train.sentiment == 0].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.849175Z",
     "start_time": "2020-08-10T22:58:04.835337Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1024)\n",
    "df_raw_kaggle_train[df_raw_kaggle_train.sentiment == 1].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TMDB Movies (2000-2020) with imdb_id](https://www.kaggle.com/hudsonmendes/tmdb-movies-20002020-with-imdb-id)\n",
    "\n",
    "In order to deliver our data warehouse, we must write user review sentiment data to our facts table. In order to achieve that, we must link films to reviews.\n",
    "\n",
    "Now, **IMDb** does have reviews in their website, however [IMDb Conditions of Use do not allow scrapping](https://www.imdb.com/conditions) as follows:\n",
    "\n",
    "> **Robots and Screen Scraping:** You may not use data mining, robots, screen scraping, or similar data gathering and extraction tools on this site, except with our express written consent as noted below.\n",
    "\n",
    "The alternative are the movie reviews available im **[TMDb (The Movie Database)](https://www.themoviedb.org/)** who provides user reviews for films.\n",
    "\n",
    "**TMDb** allows you to find movies by `imdb_id` in their API. However, downloading each film by their `imdb_id` from their API, one by one, for more than 6 million titles, would take approximately 35 days if done in series. \n",
    "\n",
    "As a solution, we are going to use another [Kaggle Dataset](https://www.kaggle.com/hudsonmendes/tmdb-movies-20002020-with-imdb-id) that provides the same data, but ideal for consumption in batch. Once again, Kaggle does not offer a public link for our dataset, so we shall download this dataset from an [S3 DataLake](https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/jcblaise/imdb-sentiment.zip).\n",
    "\n",
    "| Attribute      \t| Value                                                                                                                    \t|\n",
    "|----------------\t|--------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| Title          \t| TMDb Movies (2000-2020) with `imdb_id`                                                                                   \t|\n",
    "| Original Link  \t| [Kaggle Dataset](https://www.kaggle.com/hudsonmendes/tmdb-movies-20002020-with-imdb-id)                                  \t|\n",
    "| Data Lake Link \t| [S3 DataLake](https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/hudsonmendes/tmdb-movies-with-imdb_id.zip) \t|\n",
    "| Author         \t| [Hudson Mendes](https://www.kaggle.com/hudsonmendes)                                                                     \t|\n",
    "| Published      \t| 2020-05-20                                                                                                               \t|\n",
    "| License        \t| [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)                                                          \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.859780Z",
     "start_time": "2020-08-10T22:58:04.851995Z"
    }
   },
   "outputs": [],
   "source": [
    "tmdb_movies_url  = 'https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/hudsonmendes/tmdb-movies-with-imdb_id.zip'\n",
    "tmdb_movies_path = 'data/raw/tmdb-movies-with-imdb_id.zip'\n",
    "(tmdb_movies_url, tmdb_movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.869667Z",
     "start_time": "2020-08-10T22:58:04.863671Z"
    }
   },
   "outputs": [],
   "source": [
    "download_file(tmdb_movies_url, tmdb_movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.955388Z",
     "start_time": "2020-08-10T22:58:04.873700Z"
    }
   },
   "outputs": [],
   "source": [
    "df_raw_kaggle_train = None\n",
    "with ZipFile(tmdb_movies_path) as zip_file:\n",
    "    df_raw_kaggle_train = pd.read_json(zip_file.open('tmdb-movies-2020.json'), lines=True)\n",
    "df_raw_kaggle_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TMDB Reviews, movies released from 2000 and 2020](https://www.kaggle.com/hudsonmendes/tmdb-reviews-movies-released-from-2000-and-2020)\n",
    "\n",
    "Finally, the previous dataset allows to link **IMDb movies** to **TMDb movies**. We can now use the **TMDb reviews** that will be used for our sentiment analysis. With these we will finally have everything we need to build the facts table in our data warehouse.\n",
    "\n",
    "Unfortunately, **TMDb reviews** must also be [downloaded from the TMDb api one by one](https://developers.themoviedb.org/3/movies/get-movie-reviews). And that is not an option for larger datasets. Therefore, we will again have to rely on a [Kaggle dataset](https://www.kaggle.com/hudsonmendes/tmdb-reviews-movies-released-from-2000-and-2020) that provides with the same information, but in JSON files format\n",
    "\n",
    "| Attribute      \t| Value                                                                                                        \t|\n",
    "|----------------\t|--------------------------------------------------------------------------------------------------------------\t|\n",
    "| Title          \t| TMDB Reviews, movies released from 2000 and 2020                                                             \t|\n",
    "| Original Link  \t| [Kaggle Dataset](https://www.kaggle.com/hudsonmendes/tmdb-reviews-movies-released-from-2000-and-2020)        \t|\n",
    "| Data Lake Link \t| [S3 DataLake](https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/hudsonmendes/tmdb-reviews.zip) \t|\n",
    "| Author         \t| [Hudson Mendes](https://www.kaggle.com/hudsonmendes)                                                         \t|\n",
    "| Published      \t| 2020-05-20                                                                                                   \t|\n",
    "| License        \t| [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)                                              \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:04.997145Z",
     "start_time": "2020-08-10T22:58:04.975413Z"
    }
   },
   "outputs": [],
   "source": [
    "tmdb_reviews_url  = 'https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/hudsonmendes/tmdb-reviews.zip'\n",
    "tmdb_reviews_path = 'data/raw/tmdb-reviews.zip'\n",
    "(tmdb_reviews_url, tmdb_reviews_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:05.014316Z",
     "start_time": "2020-08-10T22:58:05.006869Z"
    }
   },
   "outputs": [],
   "source": [
    "download_file(tmdb_reviews_url, tmdb_reviews_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:05.309648Z",
     "start_time": "2020-08-10T22:58:05.268710Z"
    }
   },
   "outputs": [],
   "source": [
    "df_raw_kaggle_train = None\n",
    "with ZipFile(tmdb_reviews_path) as zip_file:\n",
    "    df_raw_kaggle_train = pd.read_json(zip_file.open('tmdb-movies-2020-reviews.json'), lines=True)\n",
    "df_raw_kaggle_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IMDB Cast (Links)](https://datasets.imdbws.com/title.principals.tsv.gz)\n",
    "\n",
    "As source for the cast, we shall use the IMDB **Principals** dataset. **Principals** are links between names and titles. Therefore, they don't have the names, just the ids.\n",
    "\n",
    "Fortunately, IMDB does provide us a link for direct download, and we shall use that link to download the `.tsv` file and process it. However the IMDb endpoint offers very low QoS (bandwidth). For that reason, we have created a copy of the file in a data lake.\n",
    "\n",
    "| Attribute      \t| Value                                                                                                        \t|\n",
    "|----------------\t|--------------------------------------------------------------------- |\n",
    "| File          \t| `title.principals.tsv.gz`                                            |\n",
    "| Original Link  \t| [IMDb Dataset](https://datasets.imdbws.com/title.principals.tsv.gz)  |\n",
    "| Download Date     | 2020-05-26                                                           |\n",
    "| License        \t| [IMDb Non-Commercial License](https://help.imdb.com/article/imdb/general-information/can-i-use-imdb-data-in-my-software/G5JTRESSHJBBHTGX?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=3aefe545-f8d3-4562-976a-e5eb47d1bb18&pf_rd_r=AMBK36FKKT9RE70H2AK5&pf_rd_s=center-1&pf_rd_t=60601&pf_rd_i=interfaces&ref_=fea_mn_lk1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:05.843747Z",
     "start_time": "2020-08-10T22:58:05.840330Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_cast_url = 'https://datasets.imdbws.com/title.principals.tsv.gz'\n",
    "imdb_cast_path = 'data/raw/imdb-cast.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:06.165487Z",
     "start_time": "2020-08-10T22:58:06.157422Z"
    }
   },
   "outputs": [],
   "source": [
    "download_file(imdb_cast_url, imdb_cast_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:06.558536Z",
     "start_time": "2020-08-10T22:58:06.524466Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imdb_cast = pd.read_csv(imdb_cast_path, sep='\\t', compression='gzip', nrows=10)\n",
    "df_imdb_cast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IMDB Cast (Names)](https://datasets.imdbws.com/name.basics.tsv.gz)\n",
    "\n",
    "Now that we got the **Princials** (links between titles and artists), we can connec the IMDB movies to **Cast Names**, by using the **IMDB Names Dataset**.\n",
    "\n",
    "Fortunately, IMDB does provide us a link for direct download, and we shall use that link to download the `.tsv` file and process it. However the IMDb endpoint offers very low QoS (bandwidth). For that reason, we have created a copy of the file in a data lake.\n",
    "\n",
    "| Attribute      \t| Value                                                                                                        \t|\n",
    "|----------------\t|--------------------------------------------------------------------- |\n",
    "| File          \t| `name.basics.tsv.gz`                                                 |\n",
    "| Original Link  \t| [IMDb Dataset](https://datasets.imdbws.com/name.basics.tsv.gz)       |\n",
    "| Download Date     | 2020-05-26                                                           |\n",
    "| License        \t| [IMDb Non-Commercial License](https://help.imdb.com/article/imdb/general-information/can-i-use-imdb-data-in-my-software/G5JTRESSHJBBHTGX?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=3aefe545-f8d3-4562-976a-e5eb47d1bb18&pf_rd_r=AMBK36FKKT9RE70H2AK5&pf_rd_s=center-1&pf_rd_t=60601&pf_rd_i=interfaces&ref_=fea_mn_lk1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:07.558635Z",
     "start_time": "2020-08-10T22:58:07.545838Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb_names_url = 'https://datasets.imdbws.com/name.basics.tsv.gz'\n",
    "imdb_names_path = 'data/raw/name.basics.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:08.082285Z",
     "start_time": "2020-08-10T22:58:08.075077Z"
    }
   },
   "outputs": [],
   "source": [
    "download_file(imdb_names_url, imdb_names_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:08.595339Z",
     "start_time": "2020-08-10T22:58:08.565913Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imdb_names = pd.read_csv(imdb_names_path, sep='\\t', compression='gzip', nrows=10)\n",
    "df_imdb_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "### Star Schema: Review Sentiments by Film\n",
    "\n",
    "![Star Schema: Film Review Sentiments](images/star-film-review-sentiment.png 'Star Schema: Film Review Sentiments')\n",
    "\n",
    "### Star Schema: Film Review Sentiments by Cast\n",
    "\n",
    "![Star Schema: Cast Review Sentiments](images/star-cast-film-review-sentiment.png 'Star Schema: Cast Review Sentiments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fact_films_review_sentiments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute                \t| Type          \t| Nullable   \t| Value                                                             \t|\n",
    "|--------------------------\t|---------------\t|------------\t|-------------------------------------------------------------------\t|\n",
    "| `date_id`                \t| `timestamp`      \t| `not null` \t| yyyy-mm-dd, foreign key to `dim_dates`                            \t|\n",
    "| `film_id`                \t| `int`         \t| `not null` \t| foreign key to `dim_films`                                        \t|\n",
    "| `review_id`              \t| `int`         \t| `not null` \t| foreign key to `dim_reviews`                                      \t|\n",
    "| `review_sentiment_class` \t| `short`       \t| `null`     \t| [-1, 1] value representing the sentiment, classified by our model \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fact_cast_review_sentiments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute                \t| Type          \t| Nullable   \t| Value                                                             \t|\n",
    "|--------------------------\t|---------------\t|------------\t|-------------------------------------------------------------------\t|\n",
    "| `date_id`                \t| `timestamp`      \t| `not null` \t| yyyy-mm-dd, foreign key to `dim_dates`                            \t|\n",
    "| `cast_id`             \t| `int`         \t| `not null` \t| foreign key to `dim_cast`                                   \t|\n",
    "| `review_id`              \t| `int`         \t| `not null` \t| foreign key to `dim_reviews`                                      \t|\n",
    "| `review_sentiment_class` \t| `short`       \t| `null`     \t| [-1, 1] value representing the sentiment, classified by our model \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dim_dates`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T20:51:51.629625Z",
     "start_time": "2020-07-21T20:51:51.620184Z"
    }
   },
   "source": [
    "| Attribute                \t| Type          \t| Nullable   \t| Value                             \t|\n",
    "|--------------------------\t|---------------\t|------------\t|-----------------------------------\t|\n",
    "| `date_id`                \t| `timestamp`      \t| `not null` \t| primary key                       \t|\n",
    "| `year`                \t| `int`         \t| `not null` \t| `year` of timestamp in int format \t|\n",
    "| `month`             \t\t| `int`         \t| `not null` \t| `month` of timestamp in int format\t|\n",
    "| `day`              \t\t| `int`         \t| `not null` \t| `day` of timestamp in int format  \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dim_films`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T20:54:31.466341Z",
     "start_time": "2020-07-21T20:54:31.457362Z"
    }
   },
   "source": [
    "| Attribute                \t| Type          \t| Nullable   \t| Value                             \t|\n",
    "|--------------------------\t|---------------\t|------------\t|-----------------------------------\t|\n",
    "| `film_id`                \t| `varchar(32)` \t| `not null` \t| `idmb_id`                           \t|\n",
    "| `title`                \t| `varchar(256)` \t| `not null` \t| the original title of the film    \t|\n",
    "| `release_year`         \t| `int`         \t| `not null` \t| `year` in which the film was released\t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dim_cast`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T20:56:08.232723Z",
     "start_time": "2020-07-21T20:56:08.222324Z"
    }
   },
   "source": [
    "| Attribute                \t| Type          \t| Nullable   \t| Value                             \t|\n",
    "|--------------------------\t|---------------\t|------------\t|-----------------------------------\t|\n",
    "| `cast_id`                \t| `varchar(32)` \t| `not null` \t| `cast_id` in the IMDB database    \t|\n",
    "| `film_id`                \t| `varchar(32)` \t| `not null` \t| `imdb_id`                         \t|\n",
    "| `full_name`            \t| `varchar(256)` \t| `not null` \t| the name of the actor or actress   \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dim_reviews`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute                \t| Type          \t| Nullable   \t| Value                             \t\t|\n",
    "|--------------------------\t|---------------\t|------------\t|---------------------------------------\t|\n",
    "| `review_id`            \t| `varchar(32)` \t| `not null` \t| `review_id` in the TMDb database  \t\t|\n",
    "| `film_id`                \t| `varchar(32)` \t| `not null` \t| `imdb_id`                         \t\t|\n",
    "| `text`                \t| `varchar(32)` \t| `not null` \t| review text for sentiment classification  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "\n",
    "\n",
    "In this section we set out the procedures requred to:\n",
    "\n",
    "1. Create Data Engineering Pipeline (EC2, Redshift, Airflow)\n",
    "2. Upload the DAG files\n",
    "3. Provide Airflow Access Links\n",
    "4. Allow Teardown of Infra-structure when complete.\n",
    "\n",
    "Once installed, the pipeline DAGs should be present by Airflow such as in the following images:\n",
    "\n",
    "![Model Training DAG](images/dag-model_training.png \"Model Training DAG\")\n",
    "\n",
    "![Model Data Warehouse DAG](images/dag-data_warehouse.png \"Model Training DAG\")\n",
    "\n",
    "![Sentiment Report](images/report.png \"Sentiment Report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components\n",
    "\n",
    "Our infra-structure is composed of:\n",
    "\n",
    "| # \t| Component        \t| Type       \t| Tech       \t| Description                                                                                                                                                                                        \t|\n",
    "|---\t|------------------\t|------------\t|------------\t|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| 1 \t| Data Lake        \t| Persistent \t| S3         \t| Cached source of raw data, used for the purpose of this project.                                                                                                                                   \t|\n",
    "| 2 \t| Airflow          \t| Ephemeral  \t| EC2        \t| Responsible for the backfill and scheduling of the DAGs responsible for (a) Performing the Sentiment Analysis over TMDb Reviews and (b) populating our Data Warehouse's dimensions and fact table. \t|\n",
    "| 3 \t| Classifier Model \t| Ephemeral  \t| Tensorflow \t| Trained once every time we need to rebuild our Data Warehouse                                                                                                                                      \t|\n",
    "| 4 \t| Data Warehouse   \t| Ephemeral  \t| Redshift   \t| Populated with (dimensions) movies, actors, reviews and our review_sentiments.                                                                                                                     \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:20.195235Z",
     "start_time": "2020-08-10T22:58:15.323476Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:20.348758Z",
     "start_time": "2020-08-10T22:58:20.199292Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings\n",
    "\n",
    "The `dw_bucket_name` must be your own, must already exist in S3, and must be unique across AWS. Please change the row below to ensure that you got your own bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:20.354283Z",
     "start_time": "2020-08-10T22:58:20.351407Z"
    }
   },
   "outputs": [],
   "source": [
    "dw_bucket_name = 'hudsonmendes-dw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:20.360733Z",
     "start_time": "2020-08-10T22:58:20.357525Z"
    }
   },
   "outputs": [],
   "source": [
    "dw_region = 'eu-west-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:20.540267Z",
     "start_time": "2020-08-10T22:58:20.364068Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:20.848761Z",
     "start_time": "2020-08-10T22:58:20.543375Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2_pem_name      = 'udacity-data-eng-nano'\n",
    "ec2_pem_path = f'./{ec2_pem_name}.pem'\n",
    "if os.path.isfile(ec2_pem_path):\n",
    "    os.remove(ec2_pem_path)\n",
    "ec2_keypair = ec2.create_key_pair(KeyName=ec2_pem_name)\n",
    "with open(ec2_pem_path, 'w+') as ec2_pem_file:\n",
    "    ec2_pem_file.write(str(ec2_keypair['KeyMaterial']))\n",
    "!chmod 400 {ec2_pem_path}\n",
    "ec2_pem_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EC2 Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:21.120241Z",
     "start_time": "2020-08-10T22:58:20.853645Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2')\n",
    "iam = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:21.579071Z",
     "start_time": "2020-08-10T22:58:21.125039Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2_role = iam.create_role(\n",
    "    Path='/',\n",
    "    RoleName='hudsonmendes-dw-movie-sentiment-analysis-ec2-role',\n",
    "    Description='',\n",
    "    MaxSessionDuration=3600,\n",
    "    AssumeRolePolicyDocument=\"\"\"{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": { \"Service\": \"ec2.amazonaws.com\"},\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\"\"\".replace('<dw_bucket>', dw_bucket_name))['Role']\n",
    "ec2_role['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:21.732501Z",
     "start_time": "2020-08-10T22:58:21.581006Z"
    }
   },
   "outputs": [],
   "source": [
    "for ec2_policy in  [\n",
    "        'arn:aws:iam::aws:policy/AmazonS3FullAccess']:\n",
    "    assert iam.attach_role_policy(\n",
    "        RoleName=ec2_role['RoleName'],\n",
    "        PolicyArn=ec2_policy)['ResponseMetadata']['HTTPStatusCode'] == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:22.011107Z",
     "start_time": "2020-08-10T22:58:21.737176Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2_instance_profile = iam.create_instance_profile(InstanceProfileName='hudsonmendes-dw-movie-sentiment-analysis-ec2-instance-profile')['InstanceProfile']\n",
    "iam.get_waiter('instance_profile_exists').wait(InstanceProfileName='hudsonmendes-dw-movie-sentiment-analysis-ec2-instance-profile')\n",
    "ec2_instance_profile['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:22.246108Z",
     "start_time": "2020-08-10T22:58:22.014341Z"
    }
   },
   "outputs": [],
   "source": [
    "assert iam.add_role_to_instance_profile(\n",
    "    InstanceProfileName=ec2_instance_profile['InstanceProfileName'],\n",
    "    RoleName=ec2_role['RoleName'])['ResponseMetadata']['HTTPStatusCode'] == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:58:23.480733Z",
     "start_time": "2020-08-10T22:58:22.252197Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2_sg = ec2.create_security_group(\n",
    "    Description='Allows 22 trafic',\n",
    "    GroupName='Airflow')\n",
    "ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=22, ToPort=22, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=8080, ToPort=8080, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=5555, ToPort=5555, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=3306, ToPort=3306, GroupId=ec2_sg['GroupId'], IpProtocol='TCP')\n",
    "ec2_sg['GroupId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T22:59:09.766409Z",
     "start_time": "2020-08-10T22:58:23.485159Z"
    }
   },
   "outputs": [],
   "source": [
    "time.sleep(30) #wait instance profile...\n",
    "ec2_ami_id = 'ami-00f6a0c18edb19300'\n",
    "ec2_spot = ec2.request_spot_instances(\n",
    "    AvailabilityZoneGroup='eu-west-2',\n",
    "    InstanceCount=1,\n",
    "    LaunchSpecification={\n",
    "        'SecurityGroupIds': [ec2_sg['GroupId']],\n",
    "        'EbsOptimized': False,\n",
    "        'KeyName': ec2_pem_name,\n",
    "        'ImageId': ec2_ami_id,\n",
    "        'InstanceType': 't3.large',\n",
    "        'IamInstanceProfile': {\n",
    "            'Arn': ec2_instance_profile['Arn']\n",
    "        },\n",
    "        \"BlockDeviceMappings\": [\n",
    "            {\n",
    "                \"DeviceName\": \"/dev/sda1\",\n",
    "                \"Ebs\": {\n",
    "                        \"DeleteOnTermination\": True,\n",
    "                        \"VolumeSize\": 30,\n",
    "                        \"Encrypted\": False,\n",
    "                        \"VolumeType\": \"gp2\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    SpotPrice='0.04',\n",
    "    Type='one-time',\n",
    "    InstanceInterruptionBehavior='terminate'\n",
    ")\n",
    "ec2_spot_id = ec2_spot['SpotInstanceRequests'][0]['SpotInstanceRequestId']\n",
    "ec2.get_waiter('spot_instance_request_fulfilled').wait(\n",
    "    SpotInstanceRequestIds=[ec2_spot_id])\n",
    "ec2_spot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:02:41.079686Z",
     "start_time": "2020-08-10T22:59:09.772765Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2_vm_id = ec2.describe_spot_instance_requests(SpotInstanceRequestIds=[ ec2_spot_id ]) \\\n",
    "    ['SpotInstanceRequests'] \\\n",
    "    [0] \\\n",
    "    ['InstanceId']\n",
    "ec2.get_waiter('instance_status_ok').wait(InstanceIds=[ ec2_vm_id ])\n",
    "ec2_vm_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:02:41.357334Z",
     "start_time": "2020-08-10T23:02:41.121665Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2_ip = ec2.allocate_address(Domain='vpc')\n",
    "[ ec2_ip['PublicIp'], ec2_ip['AllocationId'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:02:41.842470Z",
     "start_time": "2020-08-10T23:02:41.360526Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2_vm_ip = ec2.associate_address(\n",
    "     InstanceId = ec2_vm_id,\n",
    "     AllocationId = ec2_ip[\"AllocationId\"])\n",
    "[ ec2_vm_ip['AssociationId'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SSH Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:02:48.177066Z",
     "start_time": "2020-08-10T23:02:41.844941Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U paramiko\n",
    "!pip install -q -U scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:02:48.620610Z",
     "start_time": "2020-08-10T23:02:48.186559Z"
    }
   },
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import scp\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:02:48.636173Z",
     "start_time": "2020-08-10T23:02:48.631203Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ssh(ip, pem_path):\n",
    "    print(f\"ssh -i udacity-data-eng-nano.pem ubuntu@{ip}\")\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(hostname=ip, username='ubuntu', pkey=paramiko.RSAKey.from_private_key_file(pem_path))\n",
    "    return ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:02:48.663818Z",
     "start_time": "2020-08-10T23:02:48.645305Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_via_ssh(\n",
    "        ip,\n",
    "        pem_path,\n",
    "        commands,\n",
    "        display_output=False):\n",
    "    \n",
    "    ssh = get_ssh(ip, pem_path)\n",
    "    try:\n",
    "        for command in tqdm(commands):\n",
    "            stdin, stdout, stderr = ssh.exec_command(command)\n",
    "            exit_status = stdout.channel.recv_exit_status()\n",
    "            if exit_status == 0:\n",
    "                print(('ok', command))\n",
    "                if display_output:\n",
    "                    output_buffer = stdout.read().decode('utf-8')\n",
    "                    if output_buffer:\n",
    "                        print(f\">>> {output_buffer}\")\n",
    "            else:\n",
    "                error_buffer = stderr.read().decode('utf-8')\n",
    "                print(('failed', command))\n",
    "                print(f\"!!! {error_buffer}\")\n",
    "    finally:\n",
    "        ssh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:04:36.955002Z",
     "start_time": "2020-08-10T23:02:48.667917Z"
    }
   },
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'sudo apt-get -y update',\n",
    "\n",
    "        'sudo apt-get install -y libmysqlclient-dev mysql-server',\n",
    "        f\"sudo mysql -e \\\"SET GLOBAL explicit_defaults_for_timestamp = 1;\\\"\",\n",
    "        f\"sudo mysql -e \\\"CREATE DATABASE airflow;\\\"\",\n",
    "        f\"sudo mysql -e \\\"CREATE USER 'airflow'@'localhost' IDENTIFIED BY 'airflow';\\\"\",\n",
    "        f\"sudo mysql -e \\\"GRANT ALL PRIVILEGES ON airflow.* TO 'airflow'@'localhost';\\\"\",\n",
    "\n",
    "        'sudo apt-get install -y python3 python3-pip python3-setuptools',\n",
    "        'sudo pip3 install -U pip',\n",
    "        'sudo pip3 install -U apache-airflow',\n",
    "        'sudo pip3 install -U apache-airflow[mysql]',\n",
    "        'sudo pip3 install -U apache-airflow[celery]'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:04:51.081935Z",
     "start_time": "2020-08-10T23:04:36.957983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_via_ssh(ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'airflow initdb',\n",
    "        'sudo apt-get install -y crudini',\n",
    "        \"crudini --set ~/airflow/airflow.cfg core load_examples False\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg core load_default_connections False\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg core sql_alchemy_conn 'mysql://airflow:airflow@localhost/airflow'\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg core executor LocalExecutor\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg core sql_alchemy_schema airflow\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg scheduler min_file_process_interval 10\",\n",
    "        \"crudini --set ~/airflow/airflow.cfg scheduler dag_dir_list_interval 60\",\n",
    "        'airflow initdb',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:04:51.856262Z",
     "start_time": "2020-08-10T23:04:51.085875Z"
    }
   },
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'mkdir -p ~/airflow/dags'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:05:48.946198Z",
     "start_time": "2020-08-10T23:04:51.860346Z"
    }
   },
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'sudo pip3 install -U tensorflow',\n",
    "        'sudo pip3 install -U pandas',\n",
    "        'sudo pip3 install -U scikit-learn',\n",
    "        'sudo pip3 install -U numpy',\n",
    "        'sudo pip3 install -U psycopg2-binary',\n",
    "        'sudo pip3 install -U requests',\n",
    "        'sudo pip3 install -U boto3',\n",
    "        'sudo pip3 install -U matplotlib',\n",
    "        'sudo pip3 install -U reportlab'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:05:57.135994Z",
     "start_time": "2020-08-10T23:05:48.949401Z"
    }
   },
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        'airflow scheduler -D',\n",
    "        'airflow worker -D',\n",
    "        'airflow flower -D',\n",
    "        'airflow webserver -p 8080 -D'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:06:07.328300Z",
     "start_time": "2020-08-10T23:05:57.141419Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U ipython-sql\n",
    "!pip install -q -U psycopg2-binary\n",
    "!pip install -q -U boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:06:07.399590Z",
     "start_time": "2020-08-10T23:06:07.334742Z"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Redshift Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:06:07.646443Z",
     "start_time": "2020-08-10T23:06:07.404217Z"
    }
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2')\n",
    "iam = boto3.client('iam')\n",
    "redshift = boto3.client('redshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:06:08.294671Z",
     "start_time": "2020-08-10T23:06:07.649503Z"
    }
   },
   "outputs": [],
   "source": [
    "redshift_role = iam.create_role(\n",
    "    Path='/',\n",
    "    RoleName='hudsonmendes-dw-movie-sentiment-analysis-redshift-role',\n",
    "    Description='',\n",
    "    MaxSessionDuration=3600,\n",
    "    AssumeRolePolicyDocument=\"\"\"{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"redshift.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\"\"\")['Role']\n",
    "redshift_role['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:06:08.453175Z",
     "start_time": "2020-08-10T23:06:08.297953Z"
    }
   },
   "outputs": [],
   "source": [
    "for redshift_policy in  [\n",
    "        'arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess']:\n",
    "    assert iam.attach_role_policy(\n",
    "        RoleName=redshift_role['RoleName'],\n",
    "        PolicyArn=redshift_policy)['ResponseMetadata']['HTTPStatusCode'] == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:06:09.009367Z",
     "start_time": "2020-08-10T23:06:08.455957Z"
    }
   },
   "outputs": [],
   "source": [
    "redshift_sg = ec2.create_security_group(\n",
    "    Description='Allows 5432 trafic',\n",
    "    GroupName='Redshift')\n",
    "ec2.authorize_security_group_ingress(CidrIp='0.0.0.0/0', FromPort=5439, ToPort=5439, GroupId=redshift_sg['GroupId'], IpProtocol='TCP')\n",
    "redshift_sg['GroupId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:06:09.316292Z",
     "start_time": "2020-08-10T23:06:09.020868Z"
    }
   },
   "outputs": [],
   "source": [
    "redshift_ip = ec2.allocate_address(Domain='vpc')\n",
    "[ redshift_ip['PublicIp'], redshift_ip['AllocationId'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:06:09.336180Z",
     "start_time": "2020-08-10T23:06:09.325200Z"
    }
   },
   "outputs": [],
   "source": [
    "redshift_host = redshift_ip['PublicIp']\n",
    "redshift_port = 5439\n",
    "redshift_user = 'redshift'\n",
    "redshift_pass = 'r4d$hifT'\n",
    "(redshift_host, redshift_port, redshift_user, redshift_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:11:11.921882Z",
     "start_time": "2020-08-10T23:06:09.339272Z"
    }
   },
   "outputs": [],
   "source": [
    "redshift_cluster = redshift.create_cluster(\n",
    "    DBName='dw_movie_review_sentiment',\n",
    "    ClusterIdentifier='hudsonmendes-redshift',\n",
    "    ClusterType='multi-node',\n",
    "    NodeType='ra3.4xlarge',\n",
    "    NumberOfNodes=2,\n",
    "    MasterUsername=redshift_user,\n",
    "    MasterUserPassword=redshift_pass,\n",
    "    VpcSecurityGroupIds=[ redshift_sg['GroupId'] ],\n",
    "    IamRoles=[ redshift_role['Arn'] ],\n",
    "    AvailabilityZone=f'{dw_region}a',\n",
    "    ElasticIp=redshift_host,\n",
    "    PubliclyAccessible=True,\n",
    "    Encrypted=False)['Cluster']\n",
    "redshift.get_waiter('cluster_available').wait(ClusterIdentifier=redshift_cluster['ClusterIdentifier'])\n",
    "redshift_cluster['ClusterIdentifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:11:12.231784Z",
     "start_time": "2020-08-10T23:11:11.929599Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:11:12.238961Z",
     "start_time": "2020-08-10T23:11:12.234253Z"
    }
   },
   "outputs": [],
   "source": [
    "redshift_db = 'dw_movie_review_sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:11:13.150384Z",
     "start_time": "2020-08-10T23:11:12.244437Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "redshift_url = f'postgresql://{redshift_user}:{redshift_pass}@{redshift_host}:5439/{redshift_db}'\n",
    "print(f\"redshift_url = '{redshift_url}'\")\n",
    "%sql $redshift_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:19.671261Z",
     "start_time": "2020-08-10T23:12:19.464306Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists public.dim_dates;\n",
    "create table if not exists public.dim_dates\n",
    "(\n",
    "    date_id timestamp without time zone not null,\n",
    "    year    integer not null,\n",
    "    month   integer not null,\n",
    "    day     integer,\n",
    "    constraint dim_dates_pkey primary key (date_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:19.868406Z",
     "start_time": "2020-08-10T23:12:19.674504Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists public.dim_films;\n",
    "create table if not exists public.dim_films\n",
    "(\n",
    "    film_id      varchar(32)  not null,\n",
    "    date_id      timestamp    not null,\n",
    "    title        varchar(256) not null,\n",
    "    constraint dim_films_pkey primary key (film_id),\n",
    "    constraint dim_films_fkey_dates foreign key (date_id) references dim_dates (date_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:20.055983Z",
     "start_time": "2020-08-10T23:12:19.872032Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists public.dim_cast;\n",
    "create table if not exists public.dim_cast\n",
    "(\n",
    "    cast_id    varchar(32)  not null,\n",
    "    film_id    varchar(32)  not null,\n",
    "    full_name  varchar(256) not null,\n",
    "    constraint dim_cast_pkey primary key (cast_id),\n",
    "    constraint dim_cast_fkey_films foreign key (film_id) references dim_films (film_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:20.236936Z",
     "start_time": "2020-08-10T23:12:20.068602Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists public.dim_reviews;\n",
    "create table if not exists public.dim_reviews\n",
    "(\n",
    "    review_id varchar(32)    not null,\n",
    "    film_id   varchar(32)    not null,\n",
    "    text      varchar(40000) not null,\n",
    "    constraint dim_reviews_pkey primary key (review_id),\n",
    "    constraint dim_reviews_fkey_films foreign key (film_id) references dim_films (film_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:20.417043Z",
     "start_time": "2020-08-10T23:12:20.239946Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists public.fact_film_review_sentiments;\n",
    "create table if not exists public.fact_film_review_sentiments\n",
    "(\n",
    "    date_id                timestamp without time zone not null,\n",
    "    film_id                varchar(32) not null,\n",
    "    review_id              varchar(32) not null,\n",
    "    review_sentiment_class integer     not null,\n",
    "    constraint fact_films_review_sentiments_pkey primary key (date_id, film_id, review_id),\n",
    "    constraint fact_films_review_sentiments_fkey_dates foreign key (date_id) references dim_dates (date_id),\n",
    "    constraint fact_films_review_sentiments_fkey_films foreign key (film_id) references dim_films (film_id),\n",
    "    constraint fact_films_review_sentiments_fkey_reviews foreign key (review_id) references dim_reviews (review_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:20.595226Z",
     "start_time": "2020-08-10T23:12:20.419251Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists public.fact_cast_review_sentiments;\n",
    "create table if not exists public.fact_cast_review_sentiments\n",
    "(\n",
    "    date_id                timestamp without time zone not null,\n",
    "    cast_id                varchar(32) not null,\n",
    "    review_id              varchar(32) not null,\n",
    "    review_sentiment_class integer     not null,\n",
    "    constraint fact_cast_review_sentiments_pkey primary key (date_id, cast_id, review_id),\n",
    "    constraint fact_cast_review_sentiments_fkey_dates foreign key (date_id) references dim_dates (date_id),\n",
    "    constraint fact_cast_review_sentiments_fkey_cast foreign key (cast_id) references dim_cast (cast_id),\n",
    "    constraint fact_cast_review_sentiments_fkey_reviews foreign key (review_id) references dim_reviews (review_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAGs\n",
    "\n",
    "#### Sentiment Classifier Training DAG\n",
    "\n",
    "The **Sentiment Classifier Training DAG** is a simplified DAG put together to train a simple model devised by the **[model training notebook](Movie%20Review%20Classifier%20Trainer.ipynb)** that reaches 90+% accurracy at classifying movie reviews between positive and negative.\n",
    "\n",
    "The Model Training process has been coded into a single Airflow Operator. An obvious improvement to this process would be to break the model building process with [TFX (Tensorflow Extended)](https://www.tensorflow.org/tfx), which is outside the scope of this project.\n",
    "\n",
    "#### Data Warehouse Building DAG\n",
    "\n",
    "The **Data Warehouse Building DAG** is a larger DAG put together to run through all the procedures described by the [data pipeline notebook](http://localhost:8888/notebooks/DAG%20-%20Movie%20Review%20Sentiment%20Data%20Warehouse.ipynb). It uses the *sentiment classifier model* previously trained by the other DAG in order to classify sentiments of reviews and build our dimension and fact tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:11:17.317299Z",
     "start_time": "2020-08-10T23:11:14.105097Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U Pygments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:11:17.334171Z",
     "start_time": "2020-08-10T23:11:17.321460Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_python_code(code):\n",
    "    from pygments import highlight\n",
    "    from pygments.lexers import PythonLexer\n",
    "    from pygments.formatters import HtmlFormatter\n",
    "    import IPython\n",
    "    formatter = HtmlFormatter()\n",
    "    return IPython.display.HTML('<style type=\"text/css\">{}</style>{}'.format(\n",
    "        formatter.get_style_defs('.highlight'),\n",
    "        highlight(code, PythonLexer(), formatter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:11:17.362759Z",
     "start_time": "2020-08-10T23:11:17.340598Z"
    }
   },
   "outputs": [],
   "source": [
    "def upload_dag_file(ip, pem_path, file_name, family_dir='airflow', display_file=True):\n",
    "    code = None\n",
    "    file_path = f'{file_name}'\n",
    "    file_dir = os.path.dirname(file_path)\n",
    "    if display_file:\n",
    "        with open(file_path) as f:\n",
    "            code = f.read()\n",
    "        if not code:\n",
    "            return None\n",
    "    \n",
    "    ssh = get_ssh(ip, pem_path)\n",
    "    try:\n",
    "        remote_file_dir = f'~/{family_dir}/{file_dir}'\n",
    "        print(f\"scp -i udacity-data-eng-nano.pem '{file_path}' 'ubuntu@{ip}:{remote_file_dir}'\")\n",
    "        scp_client = scp.SCPClient(ssh.get_transport())\n",
    "        scp_client.put(files=[file_path], remote_path=remote_file_dir, preserve_times=True)\n",
    "    finally:\n",
    "        ssh.close()\n",
    "    \n",
    "    if display_file:\n",
    "        return print_python_code(code)\n",
    "    else:\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airflow Folder Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:11:23.021987Z",
     "start_time": "2020-08-10T23:11:17.368143Z"
    }
   },
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        f\"mkdir -p ~/pipe/imdb_sentiment_analysis/data\",\n",
    "        f\"mkdir -p ~/pipe/imdb_sentiment_analysis/model\",\n",
    "        f\"mkdir -p ~/pipe/imdb_sentiment_analysis/working\",\n",
    "        f\"mkdir -p ~/pipe/imdb_sentiment_analysis/images\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airlfow Variables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:00.265664Z",
     "start_time": "2020-08-10T23:11:23.025001Z"
    }
   },
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        f\"airflow variables --set 'model_training_data_url' '{imdb_sentiment_url}'\",\n",
    "        f\"airflow variables --set 'model_training_data_path' '/home/ubuntu/pipe/imdb_sentiment_analysis/data/imdb-sentiment.zip'\",\n",
    "        f\"airflow variables --set 'model_output_path' '/home/ubuntu/pipe/imdb_sentiment_analysis/model'\",\n",
    "        f\"airflow variables --set 'model_max_length' '30'\",\n",
    "        f\"airflow variables --set 'model_vocab_size' '10000'\",\n",
    "        f\"airflow variables --set 'model_emb_dims' '64'\",\n",
    "        f\"airflow variables --set 'model_lstm_units' '128'\",\n",
    "        f\"airflow variables --set 'model_training_batch_size' '50'\",\n",
    "        f\"airflow variables --set 'model_training_epochs' '5'\",\n",
    "        f\"airflow variables --set 's3_redshift_iam_role' '{redshift_role['Arn']}'\",\n",
    "        f\"airflow variables --set 's3_redshift_region' '{dw_region}'\",\n",
    "        f\"airflow variables --set 's3_movies_source_url' 'https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/hudsonmendes/tmdb-movies-with-imdb_id.zip'\",\n",
    "        f\"airflow variables --set 's3_movie_reviews_source_url' 'https://hudsonmendes-datalake.s3.eu-west-2.amazonaws.com/kaggle/hudsonmendes/tmdb-reviews.zip'\",\n",
    "        f\"airflow variables --set 'http_cast_source_url' 'https://datasets.imdbws.com/title.principals.tsv.gz'\",\n",
    "        f\"airflow variables --set 'http_cast_names_source_url' 'https://datasets.imdbws.com/name.basics.tsv.gz'\",\n",
    "        f\"airflow variables --set 's3_staging_bucket' '{dw_bucket_name}'\",\n",
    "        f\"airflow variables --set 's3_staging_movies_folder' 'tmdb-movies-with-imdb_id'\",\n",
    "        f\"airflow variables --set 's3_staging_movie_reviews_folder' 'tmdb-reviews'\",\n",
    "        f\"airflow variables --set 's3_staging_cast_path' 'imdb-cast/imdb-cast.tsv.gz'\",\n",
    "        f\"airflow variables --set 's3_staging_cast_names_path' 'imdb-cast/imdb-cast-names.tsv.gz'\",\n",
    "        f\"airflow variables --set 'redshift_db' '{redshift_db}'\",\n",
    "        f\"airflow variables --set 'path_images_dir' '/home/ubuntu/pipe/imdb_sentiment_analysis/images'\",\n",
    "        f\"airflow variables --set 'path_working_dir' '/home/ubuntu/pipe/imdb_sentiment_analysis/working'\",\n",
    "        f\"airflow variables --set 's3_report_bucket' '{dw_bucket_name}'\",\n",
    "        f\"airflow variables --set 's3_report_folder' 'reports-sentiment'\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airflow Connections Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:02.320194Z",
     "start_time": "2020-08-10T23:12:00.269286Z"
    }
   },
   "outputs": [],
   "source": [
    "run_via_ssh(\n",
    "    ip=ec2_ip['PublicIp'],\n",
    "    pem_path=ec2_pem_path,\n",
    "    commands=[\n",
    "        f\"\"\"airflow connections --add \\\n",
    "            --conn_id 'redshift_dw' \\\n",
    "            --conn_type 'postgres' \\\n",
    "            --conn_host '{redshift_host}' \\\n",
    "            --conn_login '{redshift_user}' \\\n",
    "            --conn_password '{redshift_pass}' \\\n",
    "            --conn_schema 'public' \\\n",
    "            --conn_port 5439\"\"\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading Operators & DAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:41:48.116522Z",
     "start_time": "2020-08-10T23:41:37.105966Z"
    }
   },
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir('./dags/')):\n",
    "    if filename.endswith(\".py\"):\n",
    "        upload_dag_file(\n",
    "            ip=ec2_ip['PublicIp'],\n",
    "            pem_path=ec2_pem_path,\n",
    "            file_name=f'dags/{filename}',\n",
    "            display_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading DAG Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:19.424599Z",
     "start_time": "2020-08-10T23:12:09.454414Z"
    }
   },
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir('./images/')):\n",
    "    if filename.endswith(\".png\"):\n",
    "        upload_dag_file(\n",
    "            ip=ec2_ip['PublicIp'],\n",
    "            pem_path=ec2_pem_path,\n",
    "            file_name=f'images/{filename}',\n",
    "            family_dir='pipe/imdb_sentiment_analysis',\n",
    "            display_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to get **Airflow** weblinks to se the:\n",
    "\n",
    "1. **Web Server** the Airflow UI, used to manage your DAGS\n",
    "2. **Flower Interface** see all workers running in this Airflow instance\n",
    "\n",
    "**Very important:** Once you have accessed the **Airflow** server, don't forget you must enable the DAGs, so they are executed and generate the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:12:19.441736Z",
     "start_time": "2020-08-10T23:12:19.428455Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"SSH      : ssh -i udacity-data-eng-nano.pem ubuntu@{ec2_ip['PublicIp']}\")\n",
    "print(f\"WebServer: http://{ec2_ip['PublicIp']}:8080\")\n",
    "print(f\"Flower   : http://{ec2_ip['PublicIp']}:5555\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T23:57:11.884287Z",
     "start_time": "2020-08-10T23:54:35.662355Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Terminating Infrastructure.\n",
    "Do you want to continue? [y/n]\"\"\"\n",
    "if input(question).lower() == 'y':\n",
    "\n",
    "    # clients\n",
    "    ec2 = boto3.client('ec2')\n",
    "    iam = boto3.client('iam')\n",
    "    redshift = boto3.client('redshift')\n",
    "    \n",
    "    # redshift\n",
    "    redshift.delete_cluster(ClusterIdentifier=redshift_cluster['ClusterIdentifier'], SkipFinalClusterSnapshot=True)\n",
    "    redshift.get_waiter('cluster_deleted').wait(ClusterIdentifier=redshift_cluster['ClusterIdentifier'])\n",
    "    ec2.delete_security_group(GroupId=redshift_sg['GroupId'])\n",
    "    ec2.release_address(AllocationId=redshift_ip['AllocationId'])\n",
    "    for attached_policy in iam.list_attached_role_policies(RoleName=redshift_role['RoleName'])['AttachedPolicies']:\n",
    "        iam.detach_role_policy(RoleName=redshift_role['RoleName'], PolicyArn=attached_policy['PolicyArn'])\n",
    "    for policy_name in iam.list_role_policies(RoleName=redshift_role['RoleName'])['PolicyNames']:\n",
    "        iam.delete_role_policy(RoleName=redshift_role['RoleName'], PolicyName=policy_name)\n",
    "    iam.delete_role(RoleName=redshift_role['RoleName'])\n",
    "    \n",
    "    # ec2\n",
    "    ec2.cancel_spot_instance_requests(SpotInstanceRequestIds=[ ec2_spot_id ])\n",
    "    ec2.terminate_instances(InstanceIds=[ ec2_vm_id ])\n",
    "    ec2.get_waiter('instance_terminated').wait(InstanceIds=[ ec2_vm_id ])\n",
    "    ec2.release_address(AllocationId=ec2_ip['AllocationId'])\n",
    "    ec2.delete_security_group(GroupId=ec2_sg['GroupId'])\n",
    "    ec2.delete_key_pair(KeyName=ec2_pem_name)\n",
    "    for attached_policy in iam.list_attached_role_policies(RoleName=ec2_role['RoleName'])['AttachedPolicies']:\n",
    "        iam.detach_role_policy(RoleName=ec2_role['RoleName'], PolicyArn=attached_policy['PolicyArn'])\n",
    "    for policy_name in iam.list_role_policies(RoleName=ec2_role['RoleName'])['PolicyNames']:\n",
    "        iam.delete_role_policy(RoleName=ec2_role['RoleName'], PolicyName=policy_name)\n",
    "    iam.remove_role_from_instance_profile(InstanceProfileName=ec2_instance_profile['InstanceProfileName'], RoleName=ec2_role['RoleName'])\n",
    "    iam.delete_instance_profile(InstanceProfileName=ec2_instance_profile['InstanceProfileName'])\n",
    "    iam.delete_role(RoleName=ec2_role['RoleName'])\n",
    "    print('Termination completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "424px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
